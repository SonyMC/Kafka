Advanced Topics Configuration - Log Cleanup Policies
-------------------------------------------------------

- Many Kafka clusters make data expirer, according to a policy
- That concept is called log cleanup

Policy 1: log.cleanup.policy=delete ( Kafka default for all user topics)
	- Delete based on age of data( default is a week)
	- Delete base don max. size of log(default is -1 = infinite)

Policy 2: log.cleanup.policy=compact(Kafka default for topic ___consumer_offsets)
	- Delete based on keys of your message 
	- Will delete old duplicate keys after the active segment is committed
	- Infinite time and space retention


		
Logup Clenaup: Why and When:
	- Deleting data from Kafka allows you to:
		- Control the size of the data on the disk, delete obsolete data
		- Overall: Limit maintenanace work on the Kafka cluster

How often does log cleanup happen?
	- Log clenaup happens on your parttion segments
	- Smaller/More segments means that log clenaup will happen more often
	- Log clenaup shouldn't happen too often => takes CPU and RAM resources
	- The cleaner checks for work every 15 secs(log.cleaner.backoff.ms)



 

Start Kafka 
-------------


(1) Disable IPV6:

WSL console: 
cmds:
sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1
sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1


(2)Start Kafka broker:


	- Start Zookeeper and Kafka

			(1) Start Zookeeper using binaries in WSL2:
			- Open WSL:
				- Start Zookeper:
					- cmd: zookeeper-server-start.sh ~/kafka_2.13-3.0.0/config/zookeeper.properties
				- Keep the window open
			


			(2) Start Kafka using binaries in another process in WSL2:
					- Start Kafka:
						- cmd: kafka-server-start.sh ~/kafka_2.13-3.0.0/config/server.properties
							_ if you get an error, run the cmd again



3) Describe topic '__consumer_offsets':
	- cmd: kafka-topics.sh --bootstrap-server localhost:9092 --topic __consumer_offsets --describe
		- You can see teh followign in the configs:

Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600






