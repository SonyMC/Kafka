Wikimedia Producer -Producer Default Partitoner & Sticky Parttioner
--------------------------------------------------------------------


Producer Default Partitoner when key !=null:
	- Key Hashing is the process of determining the mapping of a key to a partition
	- In the default Kafka parttioner, the keys are hashed using the murmur2 algorithm
		-  targetPartition = Math.abs(Utils.murmur2(Utils.murmur2(keyBytes)) % ( numPartittions - 1)
	- This means that teh same key will go to the same partittion(we already know this) .
	- However  adding partitions to a topic will completely alter the formula
	- It is most preferred to not overrid eteh behavious of teh parttioner, but it is possible to do so using parttioner.class



Producer Default Partitoner when key == null:
	- Round Robin: for KAfka 2.3 and below
	- Sticky Parttioner: for Kafka 2.4 and above
	

- Sticky Partttioner improves the performance of teh producer, escpecially high throughput when key is null

-Producer Default Partttioner Kafka <= v2.3 , Round Robin Partitioner:
	- When ther eis no parttion & no key  specified, teh default parttioner sends data in a round-robin fashion
	- This results in more batches( one batch per partition) and smaller batches( imagine with 100 parttions)
	- Smaller batches lead to more requests as well as higher latency

  
- Producer Default Partttioner Kafka >= v2.4 ,Sticky PArttioner:
	- It would be better to to havr all records sent to a single parttion and not multiple parttions to improve batching
	- The producer sticky partitioner:
		- We stick to a partition until the batch is full or linger.ms has elapsed
		- After sending the batch, teh parttion that is sticky chages
	- Larger batches and reduced latency(because larger requests, and batch.size more likely to eb reached)
	- Over time, records are still spread evenly across parttions 





Refer : https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner




- Wikimedia Recent change stream :
		
	-url: https://stream.wikimedia.org/v2/stream/recentchange
	- page keeps on gettign updates very fast
	- Is realtime and shown all changes happening in Wikimedia
	-  What is Wikimedia used for?
		- The nonprofit Wikimedia Foundation provides the essential infrastructure for free knowledge
	- What's the difference between Wikipedia and Wikimedia?
		- Based in San Francisco, the Wikimedia Foundation (WMF) is the organization that owns the domain wikipedia.org

	- We will use thsi streeam to send data into Apache Kafka 

- Wikimedia Recent Change Stats:
		- https://codepen.io/Krinkle/pen/BwEKgW?editors=1010
- Wikimedia Event Stream Demo:
		- https://esjewett.github.io/wm-eventsource-demo/s





- Classes:
	- WikiMediaChangesProducerHighThroughPut.java
	- WikimediaChangeHandler.java( implements EventHandler)






Before running the code
-----------------------



(1) Disable IPV6:

WSL console: 
cmds:
sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1
sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1


(2)Start Kafka broker:


	- Start Zookeeper and Kafka

			(1) Start Zookeeper using binaries in WSL2:
			- Open WSL:
				- Start Zookeper:
					- cmd: zookeeper-server-start.sh ~/kafka_2.13-3.0.0/config/zookeeper.properties
				- Keep the window open
			


			(2) Start Kafka using binaries in another process in WSL2:
					- Start Kafka:
						- cmd: kafka-server-start.sh ~/kafka_2.13-3.0.0/config/server.properties
							_ if you get an error, run the cmd again



(3)List Kafka topics:
		- cmd: kafka-topics.sh --bootstrap-server localhost:9092 --list

__consumer_offsets
demo_java
first_topic
java_demo
new_topic
thrid_topic
wikimedia.recentchange



(4) Create Kafka topic named 'wikimedia.recentchange' with 3 partitions & replication factor 1 :

Note: Create the topic ''wikimedia.recentchange''  only if it is hase notbeen created before

		- cmd: kafka-topics.sh --bootstrap-server localhost:9092 --topic wikimedia.recentchange --create --partitions 3 --replication-factor 1


WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic demo_java.	




Wikimedia Producer Run
------------------------



- New WSL console for a consumer from topic 'wikimedia.recentchange' to read from earliest msg and display the key as a string and value as a json
	- cmd: 
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic wikimedia.recentchange --formatter kafka.tools.DefaultMessageFormatter --property print.timestamp=true --property print.key=true --property format.key=string --property print.value=true  --property format.value=json --from-beginning		



			OR

- - New WSL console for a simple consumer from topic 'wikimedia.recentchange'  
	- cmd: 
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic wikimedia.recentchange



- We will  read from the latest 
- Nothing will be shown as we have not started our producer  

- Start producer: 
	- Run WikiMediaChangesProducerHighThroughPut.main()
		- Startup log:

				- Default Parttioner uses the StickyParttionCache.
				- Refer system library class DefaultPartitioner.java


			


- Go to WSL consumer console:
	- You can see data being consumed from the topic to which we have produced using the java code
		 

- Start Conduktor:
	- Connect to teh runnign local KAfka CLuster
	- Start a consumer to teh topic 'wikimedia.recentchange'
	- You will see losts of msgs coming through. You can see the details of teh msg ( after stoppignteh consumer) by clicking on the msg and seelcting the 'details' button 


	
