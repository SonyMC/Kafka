Kafka Connect Concepts
-------------------------

- History
	- (2013) Kafka 0.8.:
		- Topic Raelication, Lof Compction
		- Simplified producer client API
	- (Nov 2015) Kafka 0.9.x
		- Simplified high level consumer APIs, without Zookeeper dependency
		- Added security( Encryption & Authentication)
		- Kaka Connect APIs:
	- (May 2016): Kafka 0.10.0:
		- Kafka StreamsAPIs
	- (end 2016- March 2017) Kafka 0.10.1, 0.10.2:
		- Improved Connect API, Single Message Transforms API


- Why KAfka COnnect and Streams:
	- Common Kafka Use Cases
		(1) Producer API 
			Source -> Kafka
				- Kafka Connect Source
					- Allows you to easily have a source and put all this data into Kafka    
		(2) Consumer,Producer API:
			Kafka -> Kafka
				- Kafka Streams
					- Served a purpose to do transformation on Kafka topics
		(3) Consumer API:
			Kafka -> Sink
				- Kafka COnnect Sink:
					- Served the third purpose into getting data out of Kafka   	
		     Kafka -> App

	- Simplify and improve getting data in and out of Kafka

	- Programmers always want to import data from teh same sources:
		- DBs, JDBC, Couchbase, GoldenGate, SAP HAN, Blockchain, Cassandra, DynamoDB, FTP, IOT, MongoDB, MQTT, RethinnDB, Salesfonrce, Solr, SQS, Twitter, etc.
	
	- Programmers always want to store data in the same sinks:
		- S3, ElasticSearch, HDFS, JDBC, SAP HANA, DocumetnDB, Cassandra, DynamoDB,HBase, MongoDB, REdis, Solr, Splunk, Twitter

	- It is tough to achieve Fault Tolerance, Exactly Once, Distribution, Ordering 
	- Other programmers may already have done a good job and written the code for you
	- So KAfka COnnect is really don't rerite code that someone has already wriite, Reuse that connector and configur it for your need.



- Kafka Connect and Streams Architecture Design:
	- REfer diagram 'Connect&StreamsArchitecture.png'



- Connectors, Configusration, Tasks, Workers:
	- Source Connectors to get data from Common Data Sources
	- Sink Connectors to publish data in Common Data Stores
	- Make it easy for non-experienced dev to quickly get their data reliably into Kafka
	- Part of your ETL( Extract, Transform, Load) pipeline
	- Scaling made easy from small pipelines to comany wide pipelines
	- Reusable code

- Connect Concepts:
	- Kafka Connect Cluster has multiple loaded Connectors
		- Each connector is a re-usable piece of code(java jars) 	
		- Many connectors exist in the open source world, leverage them

	- Tasks = Connectors + User Configuration
		- A task is linked to a connector configuration 
		- A job configuration may spawn multiple tasks

   	- Tasks are executed by Kafka Connect Workers(servers)
		- A worker is asingle java process
		- A worker can be standalone or in a cluster




Standalone vs Distributed mode:
	- Standalone:
		- A single process runs all your conenctors and tasks
		- Configuration is bundled with you rprocess
		- Very easy to get started with, useful for dev and testing
		- Not fault tolerant, no scalability, hard to monitor

	- Distributed:
		- Multiple workers run your connectors and tasks
		- Configuration is submitted iusing a REST API
		- Easy to scale and fault tolerant( rebalancing in case a worker expires)
		- Useful for production deployment of connectors


Kafka Connect Cluster Distributed Architecture in Details:
	- Refer diagram 'KafkaConnectClusterDistributedArchitecture.png'
 
 