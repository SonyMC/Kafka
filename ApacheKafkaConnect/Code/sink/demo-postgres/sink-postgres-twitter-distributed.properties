# Basic configuration for our connector
name=sink-postgres-twitter-distributed
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
# We can have parallelism here so we have two tasks!
tasks.max=2
topics=demo-3-twitter
# the input topic has a schema, so we enable schemas conversion here too
#key.converter=org.apache.kafka.connect.json.JsonConverte
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schemas.enable=true
key.converter.schema.registry.url=http://127.0.0.1:8081
#value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schemas.enable=true
value.converter.schema.registry.url=http://127.0.0.1:8081
# JDBCSink connector specific configuration
# http://docs.confluent.io/3.2.0/connect/connect-jdbc/docs/sink_config_options.html
# Note: Port and Credentials must be the same as specified in the Docker-Compose.yml file
connection.url=jdbc:postgresql://postgres:5432/postgres
connection.user=postgres
connection.password=postgres
# upsert mode means inserts or updates
insert.mode=upsert
# We want the primary key to be Kafka topic + partition + offset as defined further below using 'pk.fields'. Refer "Data Mapping" in https://docs.confluent.io/3.2.0/connect/connect-jdbc/docs/sink_config_options.html
pk.mode=kafka
# default value but I want to highlight it:
pk.fields=__connect_topic,__connect_partition,__connect_offset
# List of comma seperated record value field names. If empty, all the record values are utilized, otherwise used to filter to the desired fields.Note: We cannot specify fileds having nested JSON values 
fields.whitelist=id,created_at,text,lang,is_retweet
# DB tables will be auto created
auto.create=true
# DB tables will auto evolve
auto.evolve=true
# Set error tolerance to all to avoid crashes
errors.tolerance = all
